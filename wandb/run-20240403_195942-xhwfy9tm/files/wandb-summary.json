{"train/loss": 0.0625, "train/grad_norm": 0.5742571949958801, "train/learning_rate": 7.446808510638298e-06, "train/epoch": 5.0, "train/global_step": 470, "_timestamp": 1712163705.466443, "_runtime": 122.67468595504761, "_step": 5, "train_runtime": 122.1553, "train_samples_per_second": 30.535, "train_steps_per_second": 3.848, "total_flos": 974716207319040.0, "train_loss": 0.1275136318612606, "test/en_loss": 0.12759391963481903, "test/en_accuracy": 0.9637980465523106, "test/en_precision": 0.8294091514882275, "test/en_recall": 0.8272042534337616, "test/en_f1": 0.8283052351375333, "test/en_classification_report": "              precision    recall  f1-score   support\n\n     ELIGION       1.00      0.75      0.86        12\n       ERSON       0.87      0.89      0.88       820\n          GE       0.95      0.93      0.94       131\n         ITY       0.76      0.76      0.76       206\n     OCATION       0.54      0.38      0.45        34\n  ORK_OF_ART       0.53      0.38      0.44        78\n      OUNTRY       0.78      0.88      0.83       368\n   ROFESSION       0.78      0.75      0.76       535\n        WARD       0.67      0.57      0.62        63\n           _       0.85      0.84      0.85      2267\n\n   micro avg       0.83      0.83      0.83      4514\n   macro avg       0.77      0.71      0.74      4514\nweighted avg       0.83      0.83      0.83      4514\n", "test/en_runtime": 1.4212, "test/en_samples_per_second": 65.437, "test/en_steps_per_second": 8.443, "_wandb": {"runtime": 123}}