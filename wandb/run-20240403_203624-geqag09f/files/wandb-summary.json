{"train/loss": 0.067, "train/grad_norm": 0.7618045210838318, "train/learning_rate": 7.446808510638298e-06, "train/epoch": 5.0, "train/global_step": 470, "_timestamp": 1712165911.665171, "_runtime": 127.54539799690247, "_step": 6, "train_runtime": 119.0716, "train_samples_per_second": 31.326, "train_steps_per_second": 3.947, "total_flos": 974716207319040.0, "train_loss": 0.13200384403796905, "accuracy": 0.9622430633169736, "precision": 0.8197347718588447, "recall": 0.8079308817013735, "f1": 0.8137900256610509, "test/loss": 0.12708917260169983, "test/accuracy": 0.9622430633169736, "test/precision": 0.8197347718588447, "test/recall": 0.8079308817013735, "test/f1": 0.8137900256610509, "test/classification_report": "              precision    recall  f1-score   support\n\n     ELIGION       0.89      0.67      0.76        12\n       ERSON       0.85      0.89      0.87       820\n          GE       0.94      0.89      0.91       131\n         ITY       0.75      0.79      0.77       206\n     OCATION       0.54      0.38      0.45        34\n  ORK_OF_ART       0.49      0.29      0.37        78\n      OUNTRY       0.83      0.86      0.85       368\n   ROFESSION       0.76      0.73      0.75       535\n        WARD       0.65      0.62      0.63        63\n           _       0.84      0.82      0.83      2267\n\n   micro avg       0.82      0.81      0.81      4514\n   macro avg       0.75      0.69      0.72      4514\nweighted avg       0.82      0.81      0.81      4514\n", "test/runtime": 1.6134, "test/samples_per_second": 57.643, "test/steps_per_second": 7.438, "_wandb": {"runtime": 128}}